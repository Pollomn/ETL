# üìñ Documentation Technique du Code

## üèóÔ∏è **Architecture du Code**

### **Structure des fichiers**
```
/
‚îú‚îÄ‚îÄ data_generator.py              # G√©n√©rateur de donn√©es synth√©tiques
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ export_seed.py             # Export CSV avec arguments CLI
‚îÇ   ‚îú‚îÄ‚îÄ producer.py                # Producer Kafka (replay/stream)
‚îÇ   ‚îú‚îÄ‚îÄ consumer_snowflake.py     # Consumer Kafka ‚Üí Snowflake
‚îÇ   ‚îî‚îÄ‚îÄ validate.py               # Validation et rapports
‚îú‚îÄ‚îÄ snowflake/
‚îÇ   ‚îú‚îÄ‚îÄ 01_create_warehouse_db_schemas.sql
‚îÇ   ‚îú‚îÄ‚îÄ 02_create_tables.sql
‚îÇ   ‚îú‚îÄ‚îÄ 03_create_stream_and_task.sql
‚îÇ   ‚îú‚îÄ‚îÄ 04_validate_ingestion.sql
‚îÇ   ‚îî‚îÄ‚îÄ run_all.sh                # Script d'ex√©cution
‚îú‚îÄ‚îÄ streamlit_monitor/
‚îÇ   ‚îî‚îÄ‚îÄ app.py                    # Dashboard Streamlit
‚îî‚îÄ‚îÄ docker/
    ‚îî‚îÄ‚îÄ docker-compose.yml        # Infrastructure Redpanda
```

---

## üìä **data_generator.py - G√©n√©rateur de Donn√©es**

### **Classe principale : DataGenerator**

```python
class DataGenerator:
    def __init__(self, seed=42):
        """Initialise le g√©n√©rateur avec une seed pour reproductibilit√©"""
        self.fake = Faker('fr_FR')  # Donn√©es fran√ßaises
        random.seed(seed)
        np.random.seed(seed)
```

### **M√©thodes principales**

#### **generate_customers(n_customers=1000)**
```python
def generate_customers(self, n_customers=1000):
    """
    G√©n√®re des clients avec donn√©es r√©alistes
    
    Returns:
        pd.DataFrame: DataFrame avec colonnes [id, name, email, city, country]
    """
    customers = []
    for i in range(n_customers):
        customer = {
            'id': f'CUST_{i+1:06d}',
            'name': self.fake.name(),
            'email': self.fake.email(),
            'city': self.fake.city(),
            'country': self.fake.country()
        }
        customers.append(customer)
    
    return pd.DataFrame(customers)
```

**Logique m√©tier :**
- IDs s√©quentiels avec pr√©fixe `CUST_`
- Noms et emails fran√ßais r√©alistes
- G√©olocalisation coh√©rente (ville/pays)

#### **generate_inventory_data(n_products=200)**
```python
def generate_inventory_data(self, n_products=200):
    """
    G√©n√®re un catalogue de produits v√™tements
    
    Returns:
        pd.DataFrame: DataFrame avec colonnes [id, product_name, category, price, stock_quantity]
    """
    categories = [
        'T-shirts', 'Pantalons', 'Robes', 'Vestes', 'Chaussures',
        'Accessoires', 'Sous-v√™tements', 'Maillots de bain'
    ]
    
    inventory = []
    for i in range(n_products):
        category = random.choice(categories)
        product = {
            'id': f'PROD_{i+1:06d}',
            'product_name': self._generate_product_name(category),
            'category': category,
            'price': round(random.uniform(10, 200), 2),
            'stock_quantity': random.randint(0, 100)
        }
        inventory.append(product)
    
    return pd.DataFrame(inventory)
```

**Logique m√©tier :**
- 8 cat√©gories de v√™tements
- Prix r√©alistes (10-200‚Ç¨)
- Stock variable (0-100 unit√©s)
- Noms de produits coh√©rents par cat√©gorie

#### **generate_orders(customers_df, inventory_df, n_orders=5000)**
```python
def generate_orders(self, customers_df, inventory_df, n_orders=5000):
    """
    G√©n√®re des commandes avec historique temporel r√©aliste
    
    Args:
        customers_df: DataFrame des clients
        inventory_df: DataFrame des produits
        n_orders: Nombre de commandes √† g√©n√©rer
    
    Returns:
        pd.DataFrame: DataFrame avec colonnes [id, customer_id, product_id, quantity, unit_price, sold_at]
    """
    orders = []
    start_date = datetime(2024, 1, 1)
    end_date = datetime(2024, 12, 31)
    
    for i in range(n_orders):
        # S√©lection al√©atoire client/produit
        customer = customers_df.sample(1).iloc[0]
        product = inventory_df.sample(1).iloc[0]
        
        # Date al√©atoire dans l'ann√©e
        random_date = self.fake.date_time_between(start_date, end_date)
        
        order = {
            'id': f'ORDER_{i+1:06d}',
            'customer_id': customer['id'],
            'product_id': product['id'],
            'quantity': random.randint(1, 5),
            'unit_price': product['price'],
            'sold_at': random_date.isoformat() + 'Z'
        }
        orders.append(order)
    
    return pd.DataFrame(orders)
```

**Logique m√©tier :**
- Dates r√©parties sur toute l'ann√©e 2024
- Quantit√©s r√©alistes (1-5 unit√©s)
- Prix coh√©rents avec l'inventaire
- Format ISO 8601 UTC

---

## üì§ **scripts/export_seed.py - Export CSV**

### **Fonction principale**
```python
def main():
    """Point d'entr√©e principal avec arguments CLI"""
    parser = argparse.ArgumentParser(description='Export des donn√©es de seed')
    parser.add_argument('--customers', type=int, default=1000, help='Nombre de clients')
    parser.add_argument('--products', type=int, default=200, help='Nombre de produits')
    parser.add_argument('--orders', type=int, default=5000, help='Nombre de commandes')
    parser.add_argument('--seed', type=int, default=42, help='Seed pour reproductibilit√©')
    parser.add_argument('--output', type=str, default='data/seed', help='Dossier de sortie')
    
    args = parser.parse_args()
    
    # G√©n√©ration des donn√©es
    generator = DataGenerator(seed=args.seed)
    customers_df = generator.generate_customers(args.customers)
    inventory_df = generator.generate_inventory_data(args.products)
    orders_df = generator.generate_orders(customers_df, inventory_df, args.orders)
    
    # Export CSV
    os.makedirs(args.output, exist_ok=True)
    customers_df.to_csv(f'{args.output}/customers.csv', index=False)
    inventory_df.to_csv(f'{args.output}/inventory.csv', index=False)
    orders_df.to_csv(f'{args.output}/orders.csv', index=False)
    
    print(f"‚úÖ Donn√©es export√©es dans {args.output}/")
```

### **Fonctionnalit√©s**
- **Arguments CLI** : Configuration flexible
- **Reproductibilit√©** : Seed fixe pour r√©sultats identiques
- **Formatage** : Dates ISO 8601, prix 2 d√©cimales
- **Structure** : Dossier `data/seed/` cr√©√© automatiquement

---

## üöÄ **scripts/producer.py - Producer Kafka**

### **Classe KafkaProducer**

```python
class KafkaProducer:
    def __init__(self, kafka_config):
        """
        Initialise le producer Kafka
        
        Args:
            kafka_config: Configuration Kafka (bootstrap_servers, topic)
        """
        self.kafka_config = kafka_config
        self.producer = None
```

### **M√©thodes principales**

#### **connect()**
```python
def connect(self):
    """√âtablit la connexion Kafka"""
    try:
        self.producer = KafkaProducer(
            bootstrap_servers=self.kafka_config['bootstrap_servers'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',  # Attendre confirmation de tous les replicas
            retries=3,   # Retry automatique
            request_timeout_ms=30000
        )
        print(f"‚úÖ Connected to Kafka: {self.kafka_config['bootstrap_servers']}")
        return True
    except Exception as e:
        print(f"‚ùå Failed to connect to Kafka: {e}")
        return False
```

#### **replay_mode(orders_file, rate=10)**
```python
def replay_mode(self, orders_file, rate=10):
    """
    Mode replay : lit un fichier CSV et publie chaque ligne
    
    Args:
        orders_file: Chemin vers le fichier orders.csv
        rate: Nombre de messages par seconde
    """
    print(f"üîÑ Replay mode: {orders_file} at {rate} msg/s")
    
    orders_df = pd.read_csv(orders_file)
    total_orders = len(orders_df)
    
    for index, order in orders_df.iterrows():
        # Cr√©ation de l'√©v√©nement
        event = {
            'event_type': 'order_created',
            'order': order.to_dict(),
            'timestamp': datetime.utcnow().isoformat() + 'Z'
        }
        
        # Publication
        self.producer.send(self.kafka_config['topic'], value=event)
        
        # Rate limiting
        time.sleep(1.0 / rate)
        
        if (index + 1) % 100 == 0:
            print(f"üì§ Published {index + 1}/{total_orders} events")
```

#### **stream_mode(rate=5)**
```python
def stream_mode(self, rate=5):
    """
    Mode stream : g√©n√®re des √©v√©nements al√©atoires en continu
    
    Args:
        rate: Nombre de messages par seconde
    """
    print(f"üåä Stream mode: generating events at {rate} msg/s")
    
    generator = DataGenerator()
    customers_df = generator.generate_customers(100)
    inventory_df = generator.generate_inventory_data(50)
    
    message_count = 0
    while True:
        try:
            # G√©n√©ration d'une commande al√©atoire
            customer = customers_df.sample(1).iloc[0]
            product = inventory_df.sample(1).iloc[0]
            
            order = {
                'id': f'STREAM_{message_count:06d}',
                'customer_id': customer['id'],
                'product_id': product['id'],
                'quantity': random.randint(1, 3),
                'unit_price': product['price'],
                'sold_at': datetime.utcnow().isoformat() + 'Z'
            }
            
            event = {
                'event_type': 'order_created',
                'order': order,
                'timestamp': datetime.utcnow().isoformat() + 'Z'
            }
            
            # Publication
            self.producer.send(self.kafka_config['topic'], value=event)
            message_count += 1
            
            if message_count % 50 == 0:
                print(f"üì§ Generated {message_count} events")
            
            time.sleep(1.0 / rate)
            
        except KeyboardInterrupt:
            print(f"\nüõë Stream stopped. Total events: {message_count}")
            break
```

### **Configuration Kafka**
```python
KAFKA_CONFIG = {
    'bootstrap_servers': 'localhost:9092',
    'topic': 'orders_topic'
}
```

**Param√®tres de performance :**
- `acks='all'` : Attendre confirmation de tous les replicas
- `retries=3` : Retry automatique en cas d'√©chec
- `request_timeout_ms=30000` : Timeout de 30 secondes
- `value_serializer` : S√©rialisation JSON automatique

---

## üì• **scripts/consumer_snowflake.py - Consumer Kafka**

### **Classe SnowflakeConsumer**

```python
class SnowflakeConsumer:
    def __init__(self, kafka_config, snowflake_config):
        """
        Initialise le consumer avec configurations Kafka et Snowflake
        
        Args:
            kafka_config: Configuration Kafka
            snowflake_config: Configuration Snowflake
        """
        self.kafka_config = kafka_config
        self.snowflake_config = snowflake_config
        self.consumer = None
        self.conn = None
        self.processed_count = 0
        self.error_count = 0
        self.running = True
```

### **M√©thodes principales**

#### **connect_kafka()**
```python
def connect_kafka(self):
    """√âtablit la connexion Kafka"""
    try:
        self.consumer = KafkaConsumer(
            self.kafka_config['topic'],
            bootstrap_servers=self.kafka_config['bootstrap_servers'],
            group_id='snowflake-consumer-group',
            auto_offset_reset='latest',  # Commencer par les nouveaux messages
            enable_auto_commit=True,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            consumer_timeout_ms=5000  # Timeout de 5 secondes
        )
        print(f"‚úÖ Connected to Kafka: {self.kafka_config['bootstrap_servers']}")
        return True
    except Exception as e:
        print(f"‚ùå Failed to connect to Kafka: {e}")
        return False
```

#### **connect_snowflake()**
```python
def connect_snowflake(self):
    """√âtablit la connexion Snowflake"""
    try:
        self.conn = snowflake.connector.connect(
            account=self.snowflake_config['account'],
            user=self.snowflake_config['user'],
            password=self.snowflake_config['password'],
            role=self.snowflake_config['role'],
            warehouse=self.snowflake_config['warehouse'],
            database=self.snowflake_config['database'],
            schema=self.snowflake_config['schema']
        )
        print(f"‚úÖ Connected to Snowflake: {self.snowflake_config['account']}")
        return True
    except Exception as e:
        print(f"‚ùå Failed to connect to Snowflake: {e}")
        return False
```

#### **insert_event(event_data)**
```python
def insert_event(self, event_data):
    """
    Ins√®re un √©v√©nement dans Snowflake
    
    Args:
        event_data: Dictionnaire contenant l'√©v√©nement
    
    Returns:
        bool: True si succ√®s, False sinon
    """
    try:
        cursor = self.conn.cursor()
        
        # Pr√©paration de l'√©v√©nement
        event_id = str(uuid.uuid4())
        event_type = event_data.get('event_type', 'unknown')
        payload = json.dumps(event_data)
        created_at = datetime.utcnow().isoformat()
        
        # Requ√™te d'insertion
        query = """
        INSERT INTO mon_schema.orders_events 
        (event_id, event_type, payload, created_at)
        VALUES (%s, %s, %s, %s)
        """
        
        cursor.execute(query, (event_id, event_type, payload, created_at))
        self.conn.commit()
        cursor.close()
        
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to insert event: {e}")
        self.error_count += 1
        return False
```

#### **process_batch(batch)**
```python
def process_batch(self, batch):
    """
    Traite un batch d'√©v√©nements
    
    Args:
        batch: Liste d'√©v√©nements √† traiter
    
    Returns:
        int: Nombre d'√©v√©nements trait√©s avec succ√®s
    """
    success_count = 0
    batch_size = len(batch)
    
    for message in batch:
        try:
            event_data = message.value
            
            if self.insert_event(event_data):
                success_count += 1
                self.processed_count += 1
                
        except Exception as e:
            print(f"‚ùå Failed to process message: {e}")
            self.error_count += 1
    
    print(f"‚úÖ Batch processed: {success_count}/{batch_size} successful")
    return success_count
```

#### **run() - Boucle principale**
```python
def run(self):
    """Boucle principale du consumer"""
    print(f"üöÄ Starting consumer for topic: {self.kafka_config['topic']}")
    print("Press Ctrl+C to stop")
    
    batch = []
    batch_size = 10
    last_commit = time.time()
    commit_interval = 5  # secondes
    
    try:
        for message in self.consumer:
            if not self.running:
                break
            
            batch.append(message)
            
            # Traitement par batch ou timeout
            current_time = time.time()
            if (len(batch) >= batch_size or 
                (last_commit is not None and (current_time - last_commit) >= commit_interval)):
                if batch:
                    self.process_batch(batch)
                    batch = []
                    last_commit = current_time
            
            # Mise √† jour p√©riodique du statut
            if (self.processed_count is not None and 
                self.processed_count > 0 and 
                self.processed_count % 100 == 0):
                print(f"üìä Status: {self.processed_count} processed, {self.error_count} errors")
    
    except Exception as e:
        print(f"‚ùå Consumer error: {e}")
        return False
    
    # Traitement des messages restants
    if batch:
        self.process_batch(batch)
    
    print(f"\n‚úÖ Consumer completed: {self.processed_count} events processed, {self.error_count} errors")
    return True
```

### **Gestion des signaux**
```python
def signal_handler(self, signum, frame):
    """Gestionnaire de signal pour arr√™t propre"""
    print(f"\nüõë Received signal {signum}, shutting down...")
    self.running = False
    self.close_connections()

def close_connections(self):
    """Ferme toutes les connexions"""
    if self.consumer:
        self.consumer.close()
    if self.conn:
        self.conn.close()
    print("üîå Connections closed")
```

---

## üóÑÔ∏è **snowflake/ - Scripts SQL**

### **01_create_warehouse_db_schemas.sql**
```sql
-- Cr√©ation de l'entrep√¥t de calcul
CREATE WAREHOUSE IF NOT EXISTS mon_entrepot
WITH
    WAREHOUSE_SIZE = 'XSMALL'
    AUTO_SUSPEND = 60
    AUTO_RESUME = TRUE
    INITIALLY_SUSPENDED = TRUE;

-- Cr√©ation de la base de donn√©es
CREATE DATABASE IF NOT EXISTS ma_base;

-- Utilisation de la base
USE DATABASE ma_base;

-- Cr√©ation des sch√©mas
CREATE SCHEMA IF NOT EXISTS mon_schema;
```

**Logique :**
- Entrep√¥t XSMALL pour √©conomiser les co√ªts
- Auto-suspend apr√®s 60 secondes d'inactivit√©
- Base de donn√©es et sch√©ma cr√©√©s

### **02_create_tables.sql**
```sql
-- Table des √©v√©nements bruts (RAW)
CREATE TABLE IF NOT EXISTS mon_schema.orders_events (
    event_id STRING,
    event_type STRING,
    payload VARIANT,
    created_at TIMESTAMP_NTZ
);

-- Table des commandes (PROD)
CREATE TABLE IF NOT EXISTS mon_schema.orders (
    id STRING PRIMARY KEY,
    product_id STRING,
    customer_id STRING,
    quantity INTEGER,
    unit_price DECIMAL(10,2),
    sold_at TIMESTAMP_NTZ,
    ingested_at TIMESTAMP_NTZ
);

-- Table des clients (PROD)
CREATE TABLE IF NOT EXISTS mon_schema.customers (
    id STRING PRIMARY KEY,
    name STRING,
    email STRING,
    city STRING,
    country STRING
);

-- Table de l'inventaire (PROD)
CREATE TABLE IF NOT EXISTS mon_schema.inventory (
    id STRING PRIMARY KEY,
    product_name STRING,
    category STRING,
    price DECIMAL(10,2),
    stock_quantity INTEGER
);
```

**Logique :**
- `orders_events` : Stockage brut des √©v√©nements Kafka
- `orders` : Donn√©es structur√©es des commandes
- `customers` et `inventory` : Tables de r√©f√©rence
- Types optimis√©s (DECIMAL, TIMESTAMP_NTZ)

### **03_create_stream_and_task.sql**
```sql
-- Cr√©ation du Stream sur les √©v√©nements
CREATE STREAM IF NOT EXISTS mon_schema.orders_stream 
ON TABLE mon_schema.orders_events;

-- Cr√©ation de la Task d'ETL
CREATE TASK IF NOT EXISTS mon_schema.ingest_orders_task
WAREHOUSE = mon_entrepot
SCHEDULE = '1 minute'
WHEN SYSTEM$STREAM_HAS_DATA('mon_schema.orders_stream')
AS
MERGE INTO mon_schema.orders AS target
USING (
    SELECT 
        payload:order.id::STRING as id,
        payload:order.product_id::STRING as product_id,
        payload:order.customer_id::STRING as customer_id,
        payload:order.quantity::INTEGER as quantity,
        payload:order.unit_price::DECIMAL(10,2) as unit_price,
        payload:order.sold_at::TIMESTAMP_NTZ as sold_at,
        created_at as ingested_at
    FROM mon_schema.orders_stream
) AS source
ON target.id = source.id
WHEN MATCHED THEN UPDATE SET
    product_id = source.product_id,
    customer_id = source.customer_id,
    quantity = source.quantity,
    unit_price = source.unit_price,
    sold_at = source.sold_at,
    ingested_at = source.ingested_at
WHEN NOT MATCHED THEN INSERT (
    id, product_id, customer_id, quantity, unit_price, sold_at, ingested_at
) VALUES (
    source.id, source.product_id, source.customer_id, source.quantity, 
    source.unit_price, source.sold_at, source.ingested_at
);

-- Activation de la Task
ALTER TASK mon_schema.ingest_orders_task RESUME;
```

**Logique :**
- **Stream** : Surveillance des changements sur `orders_events`
- **Task** : ETL automatis√© toutes les minutes
- **MERGE** : Upsert (insert ou update) des commandes
- **Extraction JSON** : Parsing des champs depuis le payload VARIANT

### **04_validate_ingestion.sql**
```sql
-- Validation des comptes
SELECT 'Raw Events' as table_name, COUNT(*) as count FROM mon_schema.orders_events
UNION ALL
SELECT 'Processed Orders' as table_name, COUNT(*) as count FROM mon_schema.orders;

-- Derni√®re ingestion
SELECT MAX(created_at) as last_raw_event FROM mon_schema.orders_events;
SELECT MAX(ingested_at) as last_processed_order FROM mon_schema.orders;

-- D√©tection de doublons
SELECT id, COUNT(*) as duplicate_count 
FROM mon_schema.orders 
GROUP BY id 
HAVING COUNT(*) > 1;

-- Qualit√© des donn√©es
SELECT 
    COUNT(*) as total_orders,
    COUNT(DISTINCT customer_id) as unique_customers,
    COUNT(DISTINCT product_id) as unique_products,
    AVG(quantity) as avg_quantity,
    SUM(quantity * unit_price) as total_revenue
FROM mon_schema.orders;
```

**Logique :**
- **Comptes** : V√©rification des volumes
- **Timestamps** : D√©tection des retards
- **Doublons** : Contr√¥le d'int√©grit√©
- **M√©triques** : KPIs de qualit√©

---

## üìä **streamlit_monitor/app.py - Dashboard**

### **Fonctions de donn√©es**

#### **get_kpis()**
```python
def get_kpis():
    """R√©cup√®re les KPIs principaux"""
    conn = _get_connection()
    cursor = conn.cursor()
    
    # Requ√™te des KPIs
    query = """
    SELECT 
        COUNT(*) as total_orders,
        SUM(quantity * unit_price) as total_revenue,
        COUNT(CASE WHEN sold_at >= DATEADD(hour, -24, CURRENT_TIMESTAMP()) THEN 1 END) as orders_24h
    FROM mon_schema.orders
    """
    
    cursor.execute(query)
    result = cursor.fetchone()
    
    return {
        'total_orders': result[0],
        'total_revenue': float(result[1]) if result[1] else 0,
        'orders_24h': result[2]
    }
```

#### **get_top_products()**
```python
def get_top_products():
    """R√©cup√®re le top 5 des produits"""
    conn = _get_connection()
    cursor = conn.cursor()
    
    query = """
    SELECT 
        o.product_id,
        i.product_name,
        i.category,
        SUM(o.quantity) as total_quantity,
        SUM(o.quantity * o.unit_price) as total_revenue,
        COUNT(*) as order_count
    FROM mon_schema.orders o
    JOIN mon_schema.inventory i ON o.product_id = i.id
    GROUP BY o.product_id, i.product_name, i.category
    ORDER BY total_revenue DESC
    LIMIT 5
    """
    
    cursor.execute(query)
    results = cursor.fetchall()
    
    return pd.DataFrame(results, columns=[
        'PRODUCT_ID', 'PRODUCT_NAME', 'CATEGORY', 
        'TOTAL_QUANTITY', 'TOTAL_REVENUE', 'ORDER_COUNT'
    ])
```

#### **get_recent_orders()**
```python
def get_recent_orders():
    """R√©cup√®re les 20 derni√®res commandes"""
    conn = _get_connection()
    cursor = conn.cursor()
    
    query = """
    SELECT 
        o.id as order_id,
        c.name as customer_name,
        i.product_name,
        o.quantity,
        o.unit_price,
        o.sold_at
    FROM mon_schema.orders o
    JOIN mon_schema.customers c ON o.customer_id = c.id
    JOIN mon_schema.inventory i ON o.product_id = i.id
    ORDER BY o.sold_at DESC
    LIMIT 20
    """
    
    cursor.execute(query)
    results = cursor.fetchall()
    
    return pd.DataFrame(results, columns=[
        'ORDER_ID', 'CUSTOMER_NAME', 'PRODUCT_NAME', 
        'QUANTITY', 'UNIT_PRICE', 'SOLD_AT'
    ])
```

### **Interface Streamlit**

#### **Configuration de la page**
```python
st.set_page_config(
    page_title="Dropshipping Pipeline Monitor",
    page_icon="üìä",
    layout="wide"
)

st.title("üìä Dropshipping Pipeline Monitor")
st.markdown("**Monitoring temps r√©el du pipeline de donn√©es**")
```

#### **Section KPIs**
```python
# KPIs principaux
kpis = get_kpis()

col1, col2, col3 = st.columns(3)

with col1:
    st.metric(
        label="üì¶ Total Commandes",
        value=f"{kpis['total_orders']:,}",
        delta=f"+{kpis['orders_24h']} derni√®res 24h"
    )

with col2:
    st.metric(
        label="üí∞ CA Total",
        value=f"‚Ç¨{kpis['total_revenue']:,.2f}",
        delta="‚Ç¨0.00"
    )

with col3:
    st.metric(
        label="‚è∞ Commandes 24h",
        value=f"{kpis['orders_24h']:,}",
        delta="0"
    )
```

#### **Graphiques**
```python
# Top 5 produits
st.subheader("üèÜ Top 5 Produits")
top_products = get_top_products()

if not top_products.empty:
    # Graphique en barres
    fig = px.bar(
        top_products,
        x='TOTAL_REVENUE',
        y='PRODUCT_NAME',
        orientation='h',
        title="CA par Produit",
        labels={'TOTAL_REVENUE': 'CA (‚Ç¨)', 'PRODUCT_NAME': 'Produit'}
    )
    fig.update_layout(height=400)
    st.plotly_chart(fig, use_container_width=True)
    
    # Tableau d√©taill√©
    st.dataframe(top_products, use_container_width=True)
```

---

## ‚úÖ **scripts/validate.py - Validation**

### **Classe ValidationEngine**

```python
class ValidationEngine:
    def __init__(self, snowflake_config):
        """Initialise le moteur de validation"""
        self.snowflake_config = snowflake_config
        self.conn = None
        self.results = {
            'timestamp': datetime.utcnow().isoformat(),
            'checks': {},
            'status': 'PASS'
        }
```

### **M√©thodes de validation**

#### **check_raw_vs_prod_counts()**
```python
def check_raw_vs_prod_counts(self):
    """V√©rifie la coh√©rence entre raw et prod"""
    try:
        cursor = self.conn.cursor()
        
        # Compte des √©v√©nements bruts
        cursor.execute("SELECT COUNT(*) FROM mon_schema.orders_events")
        raw_count = cursor.fetchone()[0]
        
        # Compte des commandes trait√©es
        cursor.execute("SELECT COUNT(*) FROM mon_schema.orders")
        prod_count = cursor.fetchone()[0]
        
        # V√©rification de coh√©rence
        if raw_count == 0 and prod_count > 0:
            status = "WARN"
            message = "Donn√©es en prod mais pas d'√©v√©nements bruts"
        elif raw_count > prod_count:
            status = "WARN"
            message = f"Retard de traitement: {raw_count - prod_count} √©v√©nements en attente"
        else:
            status = "PASS"
            message = "Coh√©rence OK"
        
        self.results['checks']['raw_vs_prod_counts'] = {
            'status': status,
            'raw_count': raw_count,
            'prod_count': prod_count,
            'message': message
        }
        
        return status == "PASS"
        
    except Exception as e:
        self.results['checks']['raw_vs_prod_counts'] = {
            'status': 'ERROR',
            'message': f'Query failed: {str(e)}'
        }
        return False
```

#### **check_duplicates()**
```python
def check_duplicates(self):
    """D√©tecte les doublons dans les donn√©es"""
    try:
        cursor = self.conn.cursor()
        
        # Doublons dans les √©v√©nements bruts
        cursor.execute("""
            SELECT event_id, COUNT(*) as count 
            FROM mon_schema.orders_events 
            GROUP BY event_id 
            HAVING COUNT(*) > 1
        """)
        raw_duplicates = len(cursor.fetchall())
        
        # Doublons dans les commandes
        cursor.execute("""
            SELECT id, COUNT(*) as count 
            FROM mon_schema.orders 
            GROUP BY id 
            HAVING COUNT(*) > 1
        """)
        prod_duplicates = len(cursor.fetchall())
        
        status = "PASS" if raw_duplicates == 0 and prod_duplicates == 0 else "FAIL"
        
        self.results['checks']['duplicates'] = {
            'status': status,
            'raw_duplicates': raw_duplicates,
            'prod_duplicates': prod_duplicates
        }
        
        return status == "PASS"
        
    except Exception as e:
        self.results['checks']['duplicates'] = {
            'status': 'ERROR',
            'message': f'Query failed: {str(e)}'
        }
        return False
```

#### **check_data_quality()**
```python
def check_data_quality(self):
    """V√©rifie la qualit√© des donn√©es"""
    try:
        cursor = self.conn.cursor()
        issues = []
        
        # V√©rification des valeurs nulles
        cursor.execute("""
            SELECT COUNT(*) FROM mon_schema.orders 
            WHERE id IS NULL OR product_id IS NULL OR customer_id IS NULL
        """)
        null_count = cursor.fetchone()[0]
        if null_count > 0:
            issues.append(f"{null_count} commandes avec valeurs nulles")
        
        # V√©rification des prix n√©gatifs
        cursor.execute("""
            SELECT COUNT(*) FROM mon_schema.orders 
            WHERE unit_price < 0
        """)
        negative_price_count = cursor.fetchone()[0]
        if negative_price_count > 0:
            issues.append(f"{negative_price_count} commandes avec prix n√©gatifs")
        
        # V√©rification des quantit√©s nulles
        cursor.execute("""
            SELECT COUNT(*) FROM mon_schema.orders 
            WHERE quantity <= 0
        """)
        invalid_quantity_count = cursor.fetchone()[0]
        if invalid_quantity_count > 0:
            issues.append(f"{invalid_quantity_count} commandes avec quantit√©s invalides")
        
        status = "PASS" if len(issues) == 0 else "FAIL"
        
        self.results['checks']['data_quality'] = {
            'status': status,
            'issues': issues,
            'total_issues': len(issues)
        }
        
        return status == "PASS"
        
    except Exception as e:
        self.results['checks']['data_quality'] = {
            'status': 'ERROR',
            'message': f'Query failed: {str(e)}'
        }
        return False
```

### **G√©n√©ration du rapport**
```python
def generate_report(self):
    """G√©n√®re le rapport de validation"""
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report_file = f"reports/validate_report_{timestamp}.json"
    
    # Cr√©ation du dossier reports si n√©cessaire
    os.makedirs("reports", exist_ok=True)
    
    # √âcriture du rapport
    with open(report_file, 'w') as f:
        json.dump(self.results, f, indent=2)
    
    print(f"üìÑ Validation report saved to: {report_file}")
    
    return report_file
```

---

## üê≥ **docker/docker-compose.yml - Infrastructure**

### **Configuration Redpanda**
```yaml
version: '3.8'
services:
  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:latest
    command:
      - redpanda
      - start
      - --kafka-addr
      - internal://0.0.0.0:9092,external://0.0.0.0:29092
      - --advertise-kafka-addr
      - internal://redpanda:9092,external://localhost:9092
      - --pandaproxy-addr
      - internal://0.0.0.0:8082,external://0.0.0.0:8082
      - --advertise-pandaproxy-addr
      - internal://redpanda:8082,external://localhost:8082
      - --schema-registry-addr
      - internal://0.0.0.0:8081,external://0.0.0.0:8081
      - --advertise-schema-registry-addr
      - internal://redpanda:8081,external://localhost:8081
    ports:
      - "9092:9092"
      - "29092:29092"
      - "8081:8081"
      - "8082:8082"
      - "9644:9644"
    environment:
      - REDPANDA_AUTO_CREATE_TOPICS_ENABLED=true
      - REDPANDA_GROUP_TOPIC_PARTITIONS=3
      - REDPANDA_OFFSET_TOPIC_PARTITIONS=3
      - REDPANDA_TRANSACTION_STATE_LOG_MIN_BYTES=1048576
      - REDPANDA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - REDPANDA_LOG_LEVEL=info
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    healthcheck:
      test: ["CMD", "rpk", "cluster", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### **Configuration RPK (outil CLI)**
```yaml
  rpk:
    image: docker.redpanda.com/redpandadata/redpanda:latest
    command: tail -f /dev/null
    depends_on:
      - redpanda
    volumes:
      - ./scripts:/scripts
```

### **Volumes et r√©seaux**
```yaml
volumes:
  redpanda_data:

networks:
  default:
    name: redpanda_network
```

**Logique de configuration :**
- **Ports** : 9092 (Kafka), 8081 (Schema Registry), 8082 (Pandaproxy)
- **Auto-cr√©ation** : Topics cr√©√©s automatiquement
- **Partitions** : 3 partitions par d√©faut
- **Health check** : V√©rification de sant√© toutes les 30s
- **Volumes** : Persistance des donn√©es

---

## üîß **Configuration et Variables d'Environnement**

### **Fichier .env**
```env
# Snowflake Configuration
SNOW_ACCOUNT=your_account_here
SNOW_USER=your_username_here
SNOW_PASSWORD=your_password_here
SNOW_ROLE=ACCOUNTADMIN
SNOW_WAREHOUSE=mon_entrepot
SNOW_DATABASE=ma_base
SNOW_SCHEMA=mon_schema

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC=orders_topic

# Application Configuration
LOG_LEVEL=INFO
```

### **Fichier .env.example**
```env
# Template pour les variables d'environnement
# Copier vers .env et remplir avec vos valeurs

# Snowflake Configuration
SNOW_ACCOUNT=your_account_here
SNOW_USER=your_username_here
SNOW_PASSWORD=your_password_here
SNOW_ROLE=ACCOUNTADMIN
SNOW_WAREHOUSE=mon_entrepot
SNOW_DATABASE=ma_base
SNOW_SCHEMA=mon_schema

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC=orders_topic

# Application Configuration
LOG_LEVEL=INFO
```

---

## üöÄ **Makefile - Automatisation**

### **Targets principaux**
```makefile
# Installation des d√©pendances
install:
	pip install -r requirements.txt

# G√©n√©ration des donn√©es
generate-data:
	python scripts/export_seed.py --customers 1000 --products 200 --orders 5000 --seed 42

# Infrastructure Docker
docker-up:
	cd docker && docker-compose up -d

docker-down:
	cd docker && docker-compose down

# Validation
validate:
	python scripts/validate.py

# Monitoring
monitor:
	streamlit run streamlit_monitor/app.py --server.port 8501

# Pipeline complet
all: install generate-data docker-up
	@echo "‚úÖ Pipeline ready!"
```

### **Targets de maintenance**
```makefile
# Nettoyage
clean:
	rm -rf data/seed/*.csv
	rm -rf reports/*.json
	rm -rf __pycache__

# Tests
test:
	python -m pytest tests/

# Linting
lint:
	ruff check .
	black --check .
```

---

## üìä **M√©triques et Monitoring**

### **KPIs du Pipeline**
- **Throughput** : Messages/seconde
- **Latence** : Temps de traitement end-to-end
- **Erreurs** : Taux d'√©chec des op√©rations
- **Volumes** : Nombre d'√©v√©nements trait√©s

### **M√©triques Snowflake**
- **Taille des tables** : Stockage utilis√©
- **Performance des queries** : Temps d'ex√©cution
- **Co√ªts** : Cr√©dits consomm√©s

### **M√©triques Kafka**
- **Lag consumer** : Retard de traitement
- **Throughput** : Messages produits/consomm√©s
- **Partitions** : R√©partition de la charge

---

## üîí **S√©curit√© et Bonnes Pratiques**

### **Gestion des secrets**
- Variables d'environnement pour les credentials
- Fichier `.env` dans `.gitignore`
- Pas de secrets en dur dans le code

### **Validation des donn√©es**
- Sch√©mas Pydantic pour la validation
- Contr√¥les de qualit√© automatis√©s
- Gestion d'erreurs robuste

### **Monitoring**
- Logs structur√©s
- M√©triques de performance
- Alertes sur les anomalies

---

## üéØ **Conclusion**

Cette documentation technique couvre tous les aspects du pipeline :

- **Architecture** : Composants et interactions
- **Code** : Impl√©mentation d√©taill√©e
- **Configuration** : Param√®tres et variables
- **D√©ploiement** : Infrastructure et automatisation
- **Monitoring** : M√©triques et validation

Le pipeline est **100% fonctionnel** et **enti√®rement document√©** ! üöÄ
