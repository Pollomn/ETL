# üìö Documentation Compl√®te - Pipeline Dropshipping

## üéØ **Vue d'ensemble du projet**

Ce projet impl√©mente un pipeline de donn√©es complet pour un site de dropshipping (v√™tements) utilisant :
- **G√©n√©ration de donn√©es** : Clients, produits, commandes
- **Streaming** : Apache Kafka/Redpanda pour les √©v√©nements temps r√©el
- **Data Warehouse** : Snowflake pour le stockage et l'ETL
- **Monitoring** : Streamlit pour le dashboard
- **Validation** : Scripts automatis√©s de contr√¥le qualit√©

## üèóÔ∏è **Architecture du Pipeline**

```
Data Generator ‚Üí CSV Export ‚Üí Kafka Producer ‚Üí Redpanda ‚Üí Kafka Consumer ‚Üí Snowflake RAW ‚Üí Stream ‚Üí Task ‚Üí Snowflake PROD ‚Üí Streamlit Dashboard
```

### **Composants :**

1. **Data Generation** : G√©n√©ration de donn√©es synth√©tiques
2. **Kafka Streaming** : Transport des √©v√©nements en temps r√©el
3. **Snowflake ETL** : Transformation et stockage des donn√©es
4. **Monitoring** : Dashboard temps r√©el et validation

---

## üìã **Pr√©requis et Installation**

### **1. Pr√©requis syst√®me**
```bash
# macOS avec Homebrew
brew install docker
brew install python3
brew install git

# Ou installation manuelle
# - Docker Desktop
# - Python 3.8+
# - Git
```

### **2. Installation des d√©pendances**
```bash
# Cloner le projet
git clone <repo-url>
cd "Projet final"

# Installer les d√©pendances Python
pip install -r requirements.txt

# Ou avec uv (recommand√©)
uv sync
```

### **3. Configuration Snowflake**
```bash
# Copier le template
cp .env.example .env

# √âditer .env avec vos credentials Snowflake
nano .env
```

**Variables d'environnement requises :**
```env
SNOW_ACCOUNT=your_account_here
SNOW_USER=your_username_here
SNOW_PASSWORD=your_password_here
SNOW_ROLE=ACCOUNTADMIN
SNOW_WAREHOUSE=mon_entrepot
SNOW_DATABASE=ma_base
SNOW_SCHEMA=mon_schema
```

---

## üöÄ **Guide d'Ex√©cution Complet**

### **√âtape 1 : G√©n√©ration des donn√©es**
```bash
# G√©n√©rer les donn√©es de base
make generate-data

# Ou manuellement
python scripts/export_seed.py --customers 1000 --products 200 --orders 5000 --seed 42
```

**R√©sultat :** Fichiers CSV cr√©√©s dans `data/seed/`
- `customers.csv` : 1000 clients
- `inventory.csv` : 200 produits
- `orders.csv` : 5000 commandes

### **√âtape 2 : D√©marrage de l'infrastructure**
```bash
# D√©marrer Redpanda (Kafka)
make docker-up

# V√©rifier le statut
docker ps
docker exec redpanda rpk cluster info
```

**R√©sultat :** Redpanda disponible sur `localhost:9092`

### **√âtape 3 : Configuration Snowflake**
```bash
# Ex√©cuter les scripts SQL
bash snowflake/run_all.sh

# Ou manuellement avec Python
python -c "
import snowflake.connector
import os
from dotenv import load_dotenv
load_dotenv()

# Connexion et ex√©cution des scripts
conn = snowflake.connector.connect(
    account=os.getenv('SNOW_ACCOUNT'),
    user=os.getenv('SNOW_USER'),
    password=os.getenv('SNOW_PASSWORD'),
    role=os.getenv('SNOW_ROLE'),
    warehouse=os.getenv('SNOW_WAREHOUSE'),
    database=os.getenv('SNOW_DATABASE'),
    schema=os.getenv('SNOW_SCHEMA')
)

# Ex√©cuter chaque script SQL
for script in ['01_create_warehouse_db_schemas.sql', '02_create_tables.sql', '03_create_stream_and_task.sql']:
    with open(f'snowflake/{script}', 'r') as f:
        sql = f.read()
        for statement in sql.split(';'):
            if statement.strip():
                conn.cursor().execute(statement)
                print(f'‚úÖ Ex√©cut√©: {script}')

conn.close()
"
```

**R√©sultat :** Tables Snowflake cr√©√©es
- `mon_schema.orders_events` (RAW)
- `mon_schema.orders` (PROD)
- `mon_schema.customers` (PROD)
- `mon_schema.inventory` (PROD)

### **√âtape 4 : Lancement du Consumer**
```bash
# Lancer le consumer en arri√®re-plan
python scripts/consumer_snowflake.py &

# V√©rifier qu'il fonctionne
ps aux | grep consumer
```

**R√©sultat :** Consumer connect√© √† Kafka et Snowflake

### **√âtape 5 : Production d'√©v√©nements**
```bash
# Mode replay (fichier existant)
python scripts/producer.py --mode replay --rate 10

# Mode stream (g√©n√©ration continue)
python scripts/producer.py --mode stream --rate 5
```

**R√©sultat :** √âv√©nements `order_created` publi√©s sur `orders_topic`

### **√âtape 6 : Monitoring**
```bash
# Lancer le dashboard Streamlit
streamlit run streamlit_monitor/app.py --server.port 8501

# Ou avec make
make monitor
```

**R√©sultat :** Dashboard accessible sur `http://localhost:8501`

### **√âtape 7 : Validation**
```bash
# Ex√©cuter les contr√¥les qualit√©
python scripts/validate.py

# V√©rifier le rapport
cat reports/validate_report_*.json
```

**R√©sultat :** Rapport de validation JSON g√©n√©r√©

---

## üîß **Commandes de Maintenance**

### **Gestion Docker**
```bash
# D√©marrer Redpanda
make docker-up

# Arr√™ter Redpanda
make docker-down

# V√©rifier les logs
docker logs redpanda

# Red√©marrer proprement
make docker-down && make docker-up
```

### **Gestion des donn√©es**
```bash
# Nettoyer les donn√©es g√©n√©r√©es
rm -rf data/seed/*.csv

# R√©g√©n√©rer les donn√©es
make generate-data

# V√©rifier les fichiers
ls -la data/seed/
```

### **Gestion Kafka**
```bash
# Lister les topics
docker exec redpanda rpk topic list

# Consommer des messages
docker exec redpanda rpk topic consume orders_topic --num 5

# Produire un message test
echo '{"test": "message"}' | docker exec -i redpanda rpk topic produce orders_topic

# Supprimer et recr√©er un topic
docker exec redpanda rpk topic delete orders_topic
docker exec redpanda rpk topic create orders_topic --partitions 3
```

### **Gestion Snowflake**
```bash
# Se connecter √† Snowflake
snowsql -a <account> -u <user> -w <warehouse> -d <database> -s <schema>

# Ex√©cuter des requ√™tes
snowsql -a <account> -u <user> -q "SELECT COUNT(*) FROM mon_schema.orders;"

# V√©rifier les streams et tasks
snowsql -a <account> -u <user> -q "SHOW STREAMS IN SCHEMA mon_schema;"
snowsql -a <account> -u <user> -q "SHOW TASKS IN SCHEMA mon_schema;"
```

---

## üìä **Monitoring et Debugging**

### **V√©rification du pipeline complet**
```bash
# 1. V√©rifier Redpanda
docker exec redpanda rpk cluster info

# 2. V√©rifier les messages Kafka
docker exec redpanda rpk topic consume orders_topic --num 3

# 3. V√©rifier Snowflake
python -c "
import snowflake.connector
import os
from dotenv import load_dotenv
load_dotenv()

conn = snowflake.connector.connect(
    account=os.getenv('SNOW_ACCOUNT'),
    user=os.getenv('SNOW_USER'),
    password=os.getenv('SNOW_PASSWORD'),
    role=os.getenv('SNOW_ROLE'),
    warehouse=os.getenv('SNOW_WAREHOUSE'),
    database=os.getenv('SNOW_DATABASE'),
    schema=os.getenv('SNOW_SCHEMA')
)

cursor = conn.cursor()

# V√©rifier les √©v√©nements bruts
cursor.execute('SELECT COUNT(*) FROM mon_schema.orders_events')
raw_count = cursor.fetchone()[0]
print(f'üì¶ √âv√©nements bruts: {raw_count}')

# V√©rifier les commandes trait√©es
cursor.execute('SELECT COUNT(*) FROM mon_schema.orders')
orders_count = cursor.fetchone()[0]
print(f'üìã Commandes totales: {orders_count}')

conn.close()
"
```

### **Logs et debugging**
```bash
# Logs Redpanda
docker logs redpanda --tail 50

# Logs du consumer
python scripts/consumer_snowflake.py 2>&1 | tee consumer.log

# Test de connectivit√© Kafka
python -c "
from kafka import KafkaConsumer
consumer = KafkaConsumer('orders_topic', bootstrap_servers='localhost:9092', consumer_timeout_ms=5000)
print('‚úÖ Kafka accessible')
consumer.close()
"
```

---

## üèóÔ∏è **Architecture D√©taill√©e des Composants**

### **1. Data Generator (`data_generator.py`)**

**Fonction :** G√©n√©ration de donn√©es synth√©tiques r√©alistes

**Fonctions principales :**
```python
def generate_customers(n_customers=1000, seed=42):
    """G√©n√®re des clients avec donn√©es r√©alistes"""
    
def generate_inventory_data(n_products=200, seed=42):
    """G√©n√®re un catalogue de produits v√™tements"""
    
def generate_orders(customers_df, inventory_df, n_orders=5000, seed=42):
    """G√©n√®re des commandes avec historique temporel"""
```

**Param√®tres configurables :**
- Nombre de clients, produits, commandes
- Seed pour reproductibilit√©
- Cat√©gories de produits
- P√©riodes temporelles

### **2. Export Script (`scripts/export_seed.py`)**

**Fonction :** Export des DataFrames en CSV avec formatage

**Fonctionnalit√©s :**
- Formatage des dates en ISO 8601 UTC
- Prix √† 2 d√©cimales
- IDs coh√©rents
- Arguments CLI configurables

**Usage :**
```bash
python scripts/export_seed.py --customers 1000 --products 200 --orders 5000 --seed 42
```

### **3. Kafka Producer (`scripts/producer.py`)**

**Fonction :** Publication d'√©v√©nements sur le topic Kafka

**Modes d'op√©ration :**
- **Replay** : Lit `orders.csv` et publie chaque ligne
- **Stream** : G√©n√®re des √©v√©nements al√©atoires en continu

**Configuration :**
```python
KAFKA_CONFIG = {
    'bootstrap_servers': 'localhost:9092',
    'topic': 'orders_topic'
}
```

**Usage :**
```bash
# Mode replay
python scripts/producer.py --mode replay --rate 10

# Mode stream
python scripts/producer.py --mode stream --rate 5
```

### **4. Kafka Consumer (`scripts/consumer_snowflake.py`)**

**Fonction :** Consommation des √©v√©nements et insertion dans Snowflake

**Architecture :**
- Connexion Kafka avec `kafka-python-ng`
- Connexion Snowflake avec `snowflake-connector-python`
- Traitement par batch pour performance
- Gestion d'erreurs robuste

**Configuration :**
```python
# Kafka
bootstrap_servers: localhost:9092
group_id: snowflake-consumer-group
auto_offset_reset: latest

# Snowflake
warehouse: mon_entrepot
database: ma_base
schema: mon_schema
```

### **5. Snowflake Schema (`snowflake/`)**

**Structure des tables :**

#### **RAW Schema (`mon_schema.orders_events`)**
```sql
CREATE TABLE mon_schema.orders_events (
    event_id STRING,
    event_type STRING,
    payload VARIANT,
    created_at TIMESTAMP_NTZ
);
```

#### **PROD Schema**
```sql
-- Commandes
CREATE TABLE mon_schema.orders (
    id STRING PRIMARY KEY,
    product_id STRING,
    customer_id STRING,
    quantity INTEGER,
    unit_price DECIMAL(10,2),
    sold_at TIMESTAMP_NTZ,
    ingested_at TIMESTAMP_NTZ
);

-- Clients
CREATE TABLE mon_schema.customers (
    id STRING PRIMARY KEY,
    name STRING,
    email STRING,
    city STRING,
    country STRING
);

-- Inventaire
CREATE TABLE mon_schema.inventory (
    id STRING PRIMARY KEY,
    product_name STRING,
    category STRING,
    price DECIMAL(10,2),
    stock_quantity INTEGER
);
```

### **6. Stream et Task (`snowflake/03_create_stream_and_task.sql`)**

**Stream :** Surveillance des changements sur `orders_events`
```sql
CREATE STREAM mon_schema.orders_stream ON TABLE mon_schema.orders_events;
```

**Task :** ETL automatis√© toutes les minutes
```sql
CREATE TASK mon_schema.ingest_orders_task
WAREHOUSE = mon_entrepot
SCHEDULE = '1 minute'
WHEN SYSTEM$STREAM_HAS_DATA('mon_schema.orders_stream')
AS
MERGE INTO mon_schema.orders AS target
USING mon_schema.orders_stream AS source
ON target.id = source.payload:order.id::STRING
WHEN MATCHED THEN UPDATE SET ...
WHEN NOT MATCHED THEN INSERT ...;
```

### **7. Streamlit Dashboard (`streamlit_monitor/app.py`)**

**Fonction :** Interface de monitoring temps r√©el

**KPIs affich√©s :**
- Total commandes
- CA total
- Commandes derni√®res 24h
- Top 5 produits (volume + CA)
- 20 derni√®res commandes

**Architecture :**
```python
def get_kpis():
    """R√©cup√®re les KPIs principaux"""
    
def get_top_products():
    """R√©cup√®re le top 5 des produits"""
    
def get_recent_orders():
    """R√©cup√®re les 20 derni√®res commandes"""
```

### **8. Validation Script (`scripts/validate.py`)**

**Fonction :** Contr√¥les qualit√© automatis√©s

**Contr√¥les effectu√©s :**
- Comparaison raw vs prod
- D√©tection de doublons
- Qualit√© des donn√©es
- Activit√© r√©cente
- Statut des tasks

**Rapport g√©n√©r√© :**
```json
{
  "timestamp": "2025-10-19T00:00:00Z",
  "checks": {
    "raw_vs_prod_counts": {"status": "PASS"},
    "duplicates": {"status": "PASS"},
    "data_quality": {"status": "PASS"},
    "recent_activity": {"status": "PASS"},
    "task_status": {"status": "PASS"}
  },
  "status": "PASS"
}
```

---

## üêõ **Troubleshooting Complet**

### **Probl√®me 1 : Consumer ne traite pas les messages**

**Sympt√¥mes :**
- Consumer se lance puis se ferme imm√©diatement
- "0 events processed" dans les logs
- Messages pr√©sents dans Kafka mais non trait√©s

**Diagnostic :**
```bash
# V√©rifier les messages dans Kafka
docker exec redpanda rpk topic consume orders_topic --num 5

# Tester la connectivit√©
python -c "
from kafka import KafkaConsumer
consumer = KafkaConsumer('orders_topic', bootstrap_servers='localhost:9092', consumer_timeout_ms=5000)
for msg in consumer:
    print(f'Message: {msg.value}')
    break
"
```

**Solutions :**
1. **Changer l'offset** : `auto_offset_reset='latest'`
2. **Timeout appropri√©** : `consumer_timeout_ms=5000`
3. **Groupe diff√©rent** : `group_id='new-group'`
4. **Recr√©er le topic** : `docker exec redpanda rpk topic delete orders_topic`

### **Probl√®me 2 : Erreurs Snowflake**

**Sympt√¥mes :**
- `Schema 'MA_BASE.RAW' does not exist`
- `Role 'POLLOM' specified does not exist`
- Connexion √©choue

**Solutions :**
```bash
# V√©rifier les credentials
cat .env

# Tester la connexion
python -c "
import snowflake.connector
import os
from dotenv import load_dotenv
load_dotenv()

conn = snowflake.connector.connect(
    account=os.getenv('SNOW_ACCOUNT'),
    user=os.getenv('SNOW_USER'),
    password=os.getenv('SNOW_PASSWORD'),
    role=os.getenv('SNOW_ROLE'),
    warehouse=os.getenv('SNOW_WAREHOUSE'),
    database=os.getenv('SNOW_DATABASE'),
    schema=os.getenv('SNOW_SCHEMA')
)
print('‚úÖ Connexion Snowflake OK')
conn.close()
"
```

### **Probl√®me 3 : Redpanda ne d√©marre pas**

**Sympt√¥mes :**
- `Cannot connect to the Docker daemon`
- `Conflict. The container name "/redpanda" is already in use`

**Solutions :**
```bash
# D√©marrer Docker Desktop
open -a Docker

# Nettoyer les containers
docker stop redpanda
docker rm redpanda

# Red√©marrer proprement
make docker-down
make docker-up
```

### **Probl√®me 4 : Dashboard Streamlit erreurs**

**Sympt√¥mes :**
- `Value of 'x' is not the name of a column`
- `SQL compilation error: invalid identifier`

**Solutions :**
1. **V√©rifier les noms de colonnes** : Snowflake utilise des majuscules
2. **V√©rifier les sch√©mas** : `mon_schema` au lieu de `prod`
3. **Tester les requ√™tes** : Ex√©cuter directement dans Snowsight

---

## üìà **M√©triques et Performance**

### **M√©triques Kafka**
```bash
# Statistiques du topic
docker exec redpanda rpk topic describe orders_topic

# Messages par partition
docker exec redpanda rpk topic consume orders_topic --num 10 --print-keys
```

### **M√©triques Snowflake**
```sql
-- Taille des tables
SELECT 
    table_name,
    row_count,
    bytes
FROM information_schema.tables 
WHERE table_schema = 'MON_SCHEMA';

-- Performance des queries
SELECT 
    query_id,
    query_text,
    execution_time,
    bytes_scanned
FROM snowflake.account_usage.query_history 
WHERE start_time > current_timestamp - interval '1 hour'
ORDER BY start_time DESC;
```

### **M√©triques du Pipeline**
```bash
# Temps de traitement
time python scripts/producer.py --mode replay --rate 100

# Throughput
docker exec redpanda rpk topic consume orders_topic --num 1000 | wc -l

# Latence end-to-end
python scripts/validate.py
```

---

## üîí **S√©curit√© et Bonnes Pratiques**

### **Gestion des secrets**
```bash
# Ne jamais committer .env
echo ".env" >> .gitignore

# Utiliser des variables d'environnement
export SNOW_PASSWORD="secret"
python scripts/consumer_snowflake.py
```

### **Validation des donn√©es**
```python
# Validation des sch√©mas
import pandas as pd
from pydantic import BaseModel

class OrderEvent(BaseModel):
    event_type: str
    order: dict
    timestamp: str

# Validation avant insertion
def validate_event(event_data):
    try:
        OrderEvent(**event_data)
        return True
    except Exception as e:
        print(f"‚ùå Validation failed: {e}")
        return False
```

### **Gestion des erreurs**
```python
# Retry logic
import time
from functools import wraps

def retry(max_attempts=3, delay=1):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise e
                    time.sleep(delay * (2 ** attempt))
            return wrapper
        return decorator
```

---

## üöÄ **D√©ploiement et Production**

### **Configuration Production**
```yaml
# docker-compose.prod.yml
version: '3.8'
services:
  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:latest
    command:
      - redpanda
      - start
      - --kafka-addr
      - internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr
      - internal://redpanda:9092,external://localhost:19092
      - --pandaproxy-addr
      - internal://0.0.0.0:8082,external://0.0.0.0:18082
      - --advertise-pandaproxy-addr
      - internal://redpanda:8082,external://localhost:18082
      - --schema-registry-addr
      - internal://0.0.0.0:8081,external://0.0.0.0:18081
    ports:
      - "19092:19092"
      - "18081:18081"
      - "18082:18082"
    volumes:
      - redpanda_data:/var/lib/redpanda/data
```

### **Monitoring Production**
```python
# Health checks
def check_kafka_health():
    try:
        consumer = KafkaConsumer(bootstrap_servers='localhost:9092', consumer_timeout_ms=1000)
        consumer.close()
        return True
    except:
        return False

def check_snowflake_health():
    try:
        conn = snowflake.connector.connect(**snowflake_config)
        conn.close()
        return True
    except:
        return False
```

### **Scaling**
```bash
# Multiple consumers
python scripts/consumer_snowflake.py --group-id consumer-1 &
python scripts/consumer_snowflake.py --group-id consumer-2 &

# Multiple producers
python scripts/producer.py --mode stream --rate 100 &
python scripts/producer.py --mode stream --rate 100 &
```

---

## üìö **R√©f√©rences et Ressources**

### **Documentation Officielle**
- [Apache Kafka](https://kafka.apache.org/documentation/)
- [Redpanda](https://docs.redpanda.com/)
- [Snowflake](https://docs.snowflake.com/)
- [Streamlit](https://docs.streamlit.io/)

### **Libraries Python**
- `kafka-python-ng` : Client Kafka
- `snowflake-connector-python` : Client Snowflake
- `streamlit` : Dashboard web
- `pandas` : Manipulation de donn√©es
- `faker` : G√©n√©ration de donn√©es

### **Outils de Debugging**
```bash
# Kafka tools
docker exec redpanda rpk topic list
docker exec redpanda rpk topic consume orders_topic --num 10

# Snowflake tools
snowsql -a <account> -u <user> -q "SHOW TABLES;"

# Python debugging
python -m pdb scripts/consumer_snowflake.py
```

---

## üéØ **Checklist de Validation Finale**

### **‚úÖ Infrastructure**
- [ ] Docker Desktop d√©marr√©
- [ ] Redpanda container actif et healthy
- [ ] Topic `orders_topic` cr√©√© avec 3 partitions
- [ ] Connexion Kafka fonctionnelle

### **‚úÖ Snowflake**
- [ ] Connexion Snowflake r√©ussie
- [ ] Tables cr√©√©es (orders_events, orders, customers, inventory)
- [ ] Stream et Task cr√©√©s et actifs
- [ ] Donn√©es de test ins√©r√©es

### **‚úÖ Pipeline**
- [ ] Consumer connect√© et en attente
- [ ] Producer g√©n√®re des √©v√©nements
- [ ] Messages trait√©s dans Snowflake
- [ ] Dashboard Streamlit fonctionnel

### **‚úÖ Validation**
- [ ] Script de validation ex√©cut√©
- [ ] Rapport JSON g√©n√©r√©
- [ ] Aucune erreur critique
- [ ] M√©triques coh√©rentes

### **‚úÖ Documentation**
- [ ] README.md complet
- [ ] Scripts document√©s
- [ ] Configuration expliqu√©e
- [ ] Troubleshooting couvert

---

## üéâ **Conclusion**

Ce pipeline de dropshipping est un exemple complet d'architecture de donn√©es moderne int√©grant :

- **G√©n√©ration de donn√©es** r√©alistes et reproductibles
- **Streaming temps r√©el** avec Kafka/Redpanda
- **Data Warehouse** avec Snowflake et ETL automatis√©
- **Monitoring** avec dashboard Streamlit
- **Validation** automatis√©e et rapports

Le projet respecte les bonnes pratiques de l'industrie et peut servir de base pour des impl√©mentations production plus complexes.

**Pipeline 100% fonctionnel et document√© !** üöÄ
