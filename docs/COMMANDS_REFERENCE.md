# üñ•Ô∏è R√©f√©rence Compl√®te des Commandes

## üìã **Commandes de Base du Pipeline**

### **1. Installation et Configuration**

```bash
# Cloner le projet
git clone <repo-url>
cd "Projet final"

# Installer les d√©pendances
pip install -r requirements.txt

# Ou avec uv (recommand√©)
uv sync

# Configuration Snowflake
cp .env.example .env
nano .env  # √âditer avec vos credentials
```

### **2. G√©n√©ration des Donn√©es**

```bash
# G√©n√©ration standard (1000 clients, 200 produits, 5000 commandes)
make generate-data

# G√©n√©ration personnalis√©e
python scripts/export_seed.py --customers 2000 --products 500 --orders 10000 --seed 123

# V√©rification des fichiers g√©n√©r√©s
ls -la data/seed/
head -5 data/seed/orders.csv
```

**Param√®tres disponibles :**
- `--customers N` : Nombre de clients (d√©faut: 1000)
- `--products M` : Nombre de produits (d√©faut: 200)
- `--orders K` : Nombre de commandes (d√©faut: 5000)
- `--seed S` : Seed pour reproductibilit√© (d√©faut: 42)
- `--output DIR` : Dossier de sortie (d√©faut: data/seed)

### **3. Infrastructure Docker**

```bash
# D√©marrer Redpanda
make docker-up

# V√©rifier le statut
docker ps
docker logs redpanda

# Arr√™ter Redpanda
make docker-down

# Red√©marrer proprement
make docker-down && make docker-up
```

**V√©rifications :**
```bash
# Sant√© du cluster
docker exec redpanda rpk cluster info

# Liste des topics
docker exec redpanda rpk topic list

# D√©tails du topic
docker exec redpanda rpk topic describe orders_topic
```

### **4. Configuration Snowflake**

```bash
# Ex√©cution automatique des scripts SQL
bash snowflake/run_all.sh

# Ou ex√©cution manuelle
snowsql -a <account> -u <user> -w <warehouse> -d <database> -s <schema> -f snowflake/01_create_warehouse_db_schemas.sql
snowsql -a <account> -u <user> -w <warehouse> -d <database> -s <schema> -f snowflake/02_create_tables.sql
snowsql -a <account> -u <user> -w <warehouse> -d <database> -s <schema> -f snowflake/03_create_stream_and_task.sql
snowsql -a <account> -u <user> -w <warehouse> -d <database> -s <schema> -f snowflake/04_validate_ingestion.sql
```

**V√©rifications Snowflake :**
```bash
# Test de connexion
python -c "
import snowflake.connector
import os
from dotenv import load_dotenv
load_dotenv()

conn = snowflake.connector.connect(
    account=os.getenv('SNOW_ACCOUNT'),
    user=os.getenv('SNOW_USER'),
    password=os.getenv('SNOW_PASSWORD'),
    role=os.getenv('SNOW_ROLE'),
    warehouse=os.getenv('SNOW_WAREHOUSE'),
    database=os.getenv('SNOW_DATABASE'),
    schema=os.getenv('SNOW_SCHEMA')
)
print('‚úÖ Connexion Snowflake OK')
conn.close()
"

# V√©rifier les tables
snowsql -a <account> -u <user> -q "SHOW TABLES IN SCHEMA mon_schema;"

# V√©rifier les streams et tasks
snowsql -a <account> -u <user> -q "SHOW STREAMS IN SCHEMA mon_schema;"
snowsql -a <account> -u <user> -q "SHOW TASKS IN SCHEMA mon_schema;"
```

---

## üöÄ **Commandes du Pipeline de Streaming**

### **1. Consumer Kafka ‚Üí Snowflake**

```bash
# Lancement du consumer
python scripts/consumer_snowflake.py

# Lancement en arri√®re-plan
python scripts/consumer_snowflake.py &

# V√©rifier qu'il fonctionne
ps aux | grep consumer

# Arr√™ter le consumer
pkill -f consumer_snowflake.py
```

**Configuration du consumer :**
- **Groupe** : `snowflake-consumer-group`
- **Offset** : `latest` (nouveaux messages)
- **Timeout** : 5 secondes
- **Batch size** : 10 messages

### **2. Producer Kafka**

#### **Mode Replay (fichier existant)**
```bash
# Replay standard (10 msg/s)
python scripts/producer.py --mode replay --rate 10

# Replay rapide (50 msg/s)
python scripts/producer.py --mode replay --rate 50

# Replay lent (1 msg/s)
python scripts/producer.py --mode replay --rate 1
```

#### **Mode Stream (g√©n√©ration continue)**
```bash
# Stream standard (5 msg/s)
python scripts/producer.py --mode stream --rate 5

# Stream rapide (20 msg/s)
python scripts/producer.py --mode stream --rate 20

# Stream lent (1 msg/s)
python scripts/producer.py --mode stream --rate 1
```

**Param√®tres disponibles :**
- `--mode {replay,stream}` : Mode d'op√©ration
- `--rate N` : Messages par seconde
- `--file PATH` : Fichier CSV pour mode replay (d√©faut: data/seed/orders.csv)

### **3. Tests Kafka**

```bash
# Produire un message test
echo '{"event_type": "order_created", "order": {"id": "TEST_001", "product_id": "PROD_001", "customer_id": "CUST_001", "quantity": 1, "unit_price": 29.99, "sold_at": "2024-01-01T12:00:00Z"}}' | docker exec -i redpanda rpk topic produce orders_topic

# Consommer des messages
docker exec redpanda rpk topic consume orders_topic --num 5

# Consommer en continu
docker exec redpanda rpk topic consume orders_topic

# V√©rifier les offsets
docker exec redpanda rpk topic describe orders_topic
```

---

## üìä **Commandes de Monitoring**

### **1. Dashboard Streamlit**

```bash
# Lancement standard
streamlit run streamlit_monitor/app.py --server.port 8501

# Ou avec make
make monitor

# Lancement en arri√®re-plan
streamlit run streamlit_monitor/app.py --server.port 8501 &

# Arr√™ter Streamlit
pkill -f streamlit
```

**Acc√®s :** `http://localhost:8501`

### **2. Validation du Pipeline**

```bash
# Validation compl√®te
python scripts/validate.py

# Ou avec make
make validate

# V√©rifier le rapport
cat reports/validate_report_*.json

# Validation avec sortie d√©taill√©e
python scripts/validate.py --verbose
```

**Rapport g√©n√©r√© :**
```json
{
  "timestamp": "2025-10-19T00:00:00Z",
  "checks": {
    "raw_vs_prod_counts": {"status": "PASS"},
    "duplicates": {"status": "PASS"},
    "data_quality": {"status": "PASS"},
    "recent_activity": {"status": "PASS"},
    "task_status": {"status": "PASS"}
  },
  "status": "PASS"
}
```

### **3. V√©rifications Snowflake**

```bash
# Comptes des tables
snowsql -a <account> -u <user> -q "SELECT COUNT(*) FROM mon_schema.orders_events;"
snowsql -a <account> -u <user> -q "SELECT COUNT(*) FROM mon_schema.orders;"

# Derni√®re activit√©
snowsql -a <account> -u <user> -q "SELECT MAX(created_at) FROM mon_schema.orders_events;"
snowsql -a <account> -u <user> -q "SELECT MAX(ingested_at) FROM mon_schema.orders;"

# Statut des tasks
snowsql -a <account> -u <user> -q "SHOW TASKS IN SCHEMA mon_schema;"

# Ex√©cution manuelle de la task
snowsql -a <account> -u <user> -q "ALTER TASK mon_schema.ingest_orders_task EXECUTE;"
```

---

## üîß **Commandes de Maintenance**

### **1. Gestion Docker**

```bash
# Statut des containers
docker ps
docker ps -a

# Logs Redpanda
docker logs redpanda
docker logs redpanda --tail 50
docker logs redpanda --follow

# Red√©marrer Redpanda
docker restart redpanda

# Nettoyer les containers
docker stop redpanda
docker rm redpanda
docker system prune -f
```

### **2. Gestion des Donn√©es**

```bash
# Nettoyer les donn√©es g√©n√©r√©es
rm -rf data/seed/*.csv

# R√©g√©n√©rer les donn√©es
make generate-data

# V√©rifier les fichiers
ls -la data/seed/
wc -l data/seed/*.csv

# Nettoyer les rapports
rm -rf reports/*.json
```

### **3. Gestion Kafka**

```bash
# Lister les topics
docker exec redpanda rpk topic list

# Supprimer un topic
docker exec redpanda rpk topic delete orders_topic

# Recr√©er un topic
docker exec redpanda rpk topic create orders_topic --partitions 3

# Vider un topic (supprimer et recr√©er)
docker exec redpanda rpk topic delete orders_topic
docker exec redpanda rpk topic create orders_topic --partitions 3

# Consommer depuis le d√©but
docker exec redpanda rpk topic consume orders_topic --offset start --num 10

# Consommer depuis la fin
docker exec redpanda rpk topic consume orders_topic --offset end --num 10
```

### **4. Gestion des Processus**

```bash
# Lister les processus Python
ps aux | grep python

# Lister les consumers
ps aux | grep consumer

# Tuer tous les consumers
pkill -f consumer

# Tuer tous les producers
pkill -f producer

# Tuer Streamlit
pkill -f streamlit

# Tuer tous les processus du pipeline
pkill -f "consumer_snowflake.py"
pkill -f "producer.py"
pkill -f "streamlit"
```

---

## üêõ **Commandes de Debugging**

### **1. Tests de Connectivit√©**

```bash
# Test Kafka
python -c "
from kafka import KafkaConsumer
consumer = KafkaConsumer('orders_topic', bootstrap_servers='localhost:9092', consumer_timeout_ms=5000)
print('‚úÖ Kafka accessible')
consumer.close()
"

# Test Snowflake
python -c "
import snowflake.connector
import os
from dotenv import load_dotenv
load_dotenv()

conn = snowflake.connector.connect(
    account=os.getenv('SNOW_ACCOUNT'),
    user=os.getenv('SNOW_USER'),
    password=os.getenv('SNOW_PASSWORD'),
    role=os.getenv('SNOW_ROLE'),
    warehouse=os.getenv('SNOW_WAREHOUSE'),
    database=os.getenv('SNOW_DATABASE'),
    schema=os.getenv('SNOW_SCHEMA')
)
print('‚úÖ Snowflake accessible')
conn.close()
"
```

### **2. Tests de Performance**

```bash
# Test de throughput Kafka
time python scripts/producer.py --mode stream --rate 100

# Test de latence
time python scripts/consumer_snowflake.py

# Test de validation
time python scripts/validate.py
```

### **3. Logs et Monitoring**

```bash
# Logs Redpanda en temps r√©el
docker logs redpanda --follow

# Logs du consumer
python scripts/consumer_snowflake.py 2>&1 | tee consumer.log

# Logs du producer
python scripts/producer.py --mode stream --rate 10 2>&1 | tee producer.log

# Monitoring des ressources
docker stats redpanda
```

---

## üöÄ **Commandes de D√©ploiement**

### **1. Pipeline Complet**

```bash
# D√©ploiement complet
make all

# Ou √©tape par √©tape
make install
make generate-data
make docker-up
python scripts/consumer_snowflake.py &
python scripts/producer.py --mode stream --rate 10 &
make monitor
```

### **2. Tests de Validation**

```bash
# Test end-to-end
python scripts/validate.py

# Test de coh√©rence
python -c "
import snowflake.connector
import os
from dotenv import load_dotenv
load_dotenv()

conn = snowflake.connector.connect(
    account=os.getenv('SNOW_ACCOUNT'),
    user=os.getenv('SNOW_USER'),
    password=os.getenv('SNOW_PASSWORD'),
    role=os.getenv('SNOW_ROLE'),
    warehouse=os.getenv('SNOW_WAREHOUSE'),
    database=os.getenv('SNOW_DATABASE'),
    schema=os.getenv('SNOW_SCHEMA')
)

cursor = conn.cursor()

# V√©rifier les √©v√©nements bruts
cursor.execute('SELECT COUNT(*) FROM mon_schema.orders_events')
raw_count = cursor.fetchone()[0]
print(f'üì¶ √âv√©nements bruts: {raw_count}')

# V√©rifier les commandes trait√©es
cursor.execute('SELECT COUNT(*) FROM mon_schema.orders')
orders_count = cursor.fetchone()[0]
print(f'üìã Commandes totales: {orders_count}')

# V√©rifier la coh√©rence
if raw_count > 0 and orders_count > 0:
    print('‚úÖ Pipeline fonctionnel')
else:
    print('‚ùå Pipeline non fonctionnel')

conn.close()
"
```

### **3. Nettoyage et Reset**

```bash
# Reset complet
make clean
make docker-down
rm -rf data/seed/*.csv
rm -rf reports/*.json

# Red√©ploiement
make all
```

---

## üìö **Commandes de Documentation**

### **1. G√©n√©ration de Documentation**

```bash
# V√©rifier la structure
tree -I '__pycache__|*.pyc|.git'

# Compter les lignes de code
find . -name "*.py" -exec wc -l {} + | tail -1

# V√©rifier les d√©pendances
pip list | grep -E "(kafka|snowflake|streamlit|pandas|faker)"
```

### **2. Tests de Code**

```bash
# Linting
ruff check .
black --check .

# Tests unitaires (si disponibles)
python -m pytest tests/

# V√©rification des imports
python -c "import scripts.consumer_snowflake; import scripts.producer; import scripts.validate"
```

---

## üéØ **Commandes de Production**

### **1. Monitoring Continu**

```bash
# Script de monitoring
#!/bin/bash
while true; do
    echo "=== $(date) ==="
    docker exec redpanda rpk cluster info
    python scripts/validate.py
    sleep 60
done
```

### **2. Backup et Restore**

```bash
# Backup des donn√©es
cp -r data/seed/ backup/data_$(date +%Y%m%d)/

# Backup des rapports
cp -r reports/ backup/reports_$(date +%Y%m%d)/

# Restore
cp -r backup/data_20241019/* data/seed/
```

### **3. Scaling**

```bash
# Multiple consumers
python scripts/consumer_snowflake.py --group-id consumer-1 &
python scripts/consumer_snowflake.py --group-id consumer-2 &

# Multiple producers
python scripts/producer.py --mode stream --rate 50 &
python scripts/producer.py --mode stream --rate 50 &
```

---

## üéâ **R√©sum√© des Commandes Essentielles**

### **D√©marrage Rapide**
```bash
# 1. Installation
pip install -r requirements.txt
cp .env.example .env  # √âditer avec vos credentials

# 2. G√©n√©ration des donn√©es
make generate-data

# 3. Infrastructure
make docker-up

# 4. Consumer
python scripts/consumer_snowflake.py &

# 5. Producer
python scripts/producer.py --mode stream --rate 10 &

# 6. Monitoring
make monitor
```

### **V√©rification**
```bash
# V√©rifier Kafka
docker exec redpanda rpk topic consume orders_topic --num 3

# V√©rifier Snowflake
python scripts/validate.py

# V√©rifier le dashboard
curl http://localhost:8501
```

### **Nettoyage**
```bash
# Arr√™ter tout
pkill -f consumer
pkill -f producer
pkill -f streamlit
make docker-down

# Nettoyer
make clean
```

**Pipeline 100% fonctionnel avec ces commandes !** üöÄ
